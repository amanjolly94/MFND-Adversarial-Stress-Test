# MFND-Adversarial-Stress-Test
official implementation for the paper, "Beyond the Surface: Adversarial Stress Testing of Multimodal Fake News Detection Models with LLM-Driven Perturbations
Of course. Based on the comprehensive information in the provided paper, here is a professional `README.md` file for the corresponding code repository.

-----

# Beyond the Surface: Adversarial Stress Testing of Multimodal Fake News Detection Models with LLM-Driven Perturbations

This repository contains the official implementation for the paper, "Beyond the Surface: Adversarial Stress Testing of Multimodal Fake News Detection Models with LLM-Driven Perturbations".

In this work, we present a comprehensive and rigorous evaluation of three canonical Multimodal Fake News Detection (MFND) architectures—LongBERT-VGG, CNN-VGG, and LSTM-VGG—across three heterogeneous benchmark datasets. We go beyond conventional evaluation by subjecting these models to a broad suite of 17 adversarial attacks, including novel instruction-tuned Large Language Model (LLM)-generated perturbations from Llama 3 and DeepSeek[cite: 3, 5, 6, 36]. Our findings expose critical limitations in current MFND systems and highlight the urgent need for more robust architectures[cite: 10].

![Untitled](https://github.com/user-attachments/assets/d2be1cf2-02aa-4011-bcec-d57ce0da6ff4)


## Key Contributions

1.  **Comprehensive Adversarial Benchmark**: We test three canonical MFND architectures (LongBERT-VGG, CNN-VGG, LSTM-VGG) on three diverse datasets (D1, RECOVERY, PolitiFact) against an extensive suite of 17 adversarial attacks[cite: 36].
2.  **Novel LLM-Generated Attacks**: We introduce and evaluate a new class of textual adversarial attacks generated by fine-tuned Llama 3 8B and DeepSeek-LLM 7B Chat models, providing insights into the evolving threat landscape posed by generative AI[cite: 37].
3.  **Systematic Modality Ablation**: We perform a detailed analysis to quantify the impact of removing text or image inputs during evaluation, revealing critical, context-dependent modality dependencies and model vulnerabilities[cite: 38].

## Architectures Evaluated

We evaluate three foundational MFND architectures that combine different text encoders with a pre-trained VGG19 image feature extractor[cite: 141]. These models use a late-fusion mechanism with self-attention before concatenation[cite: 145].

> *Fig 3. from the paper, showing the schematic for the LongBERT-VGG, CNN-VGG, and LSTM-VGG models[cite: 151, 152].*

## Setup and Installation

### Prerequisites

  * Python 3.8+
  * PyTorch
  * CUDA-enabled GPU (required for training and attack generation) [cite: 182]

### Installation

1.  **Clone the repository:**

    ```bash
    git clone https://github.com/your-username/MFND-Adversarial-Stress-Test.git
    cd MFND-Adversarial-Stress-Test
    ```

2.  **Install dependencies:**
    It is recommended to use a virtual environment.

    ```bash
    pip install -r requirements.txt
    ```

    *(Note: Please populate `requirements.txt` with the necessary libraries, such as `torch`, `torchvision`, `transformers`, `numpy`, `pandas`, `scikit-learn`, etc.)*

## Datasets

This study utilizes three distinct public datasets. You need to download them from their original sources and structure them as described below.

1.  **D1 Dataset**: A balanced dataset where many fake images contain explicit watermarks[cite: 159, 160].
      * **Source**: As referenced in [9].
2.  **RECOVERY Dataset**: A highly imbalanced dataset reflecting real-world fake news distributions[cite: 161].
      * **Source**: [10]
3.  **PolitiFact Dataset**: A moderately imbalanced dataset curated from the PolitiFact fact-checking website[cite: 164, 165].
      * **Source**: [11]

### Data Structure

Please organize the datasets in the following directory structure:

```
data/
├── D1/
│   ├── train/
│   ├── val/
│   └── test/
├── RECOVERY/
│   ├── train/
│   ├── val/
│   └── test/
└── PolitiFact/
    ├── train/
    ├── val/
    └── test/
```

## Running the Experiments

The following sections detail how to reproduce the experiments from the paper.

### 1\. Model Training

Train the MFND models on each dataset. Hyperparameters from the paper (Table 5) are set as defaults[cite: 185].

```bash
# Example: Train CNN-VGG on the D1 dataset
python train.py --model CNNVGG --dataset D1 --learning_rate 1e-4 --batch_size 16 --output_dir ./saved_models

# Example: Train LongBERT-VGG on the PolitiFact dataset
python train.py --model BERTVGG --dataset PolitiFact --learning_rate 3e-5 --batch_size 8 --output_dir ./saved_models
```

### 2\. Adversarial Attack Generation

Generate adversarial examples for a trained model using the test set.

  * **Image Attacks** (e.g., PGD, Pixle, MIFGSM) [cite: 121]
    ```bash
    # Example: Generate PGD attacks for the trained CNN-VGG D1 model
    python generate_attacks.py --model_path ./saved_models/CNNVGG_D1.pt --dataset D1 --attack_type PGD --attack_modality image --output_dir ./perturbed_data
    ```
  * **Text Attacks** (e.g., TextBugger, TextFooler, LLM-based) [cite: 91]
    ```bash
    # Example: Generate TextFooler attacks
    python generate_attacks.py --model_path ./saved_models/CNNVGG_D1.pt --dataset D1 --attack_type TextFooler --attack_modality text --output_dir ./perturbed_data

    # Example: Generate DeepSeek Semantic (DS-Sem) attacks
    python generate_attacks.py --model_path ./saved_models/CNNVGG_D1.pt --dataset D1 --attack_type DS-Sem --attack_modality text --output_dir ./perturbed_data
    ```

### 3\. Evaluation

Evaluate a trained model on clean or perturbed data.

  * **Baseline Evaluation (on clean test data):**
    ```bash
    python evaluate.py --model_path ./saved_models/CNNVGG_D1.pt --dataset D1
    ```
  * **Evaluation on Adversarial Data:**
    ```bash
    python evaluate.py --model_path ./saved_models/CNNVGG_D1.pt --dataset D1 --perturbed_data_path ./perturbed_data/PGD
    ```

### 4\. Modality Ablation Analysis

Evaluate model performance when one modality is removed[cite: 189].

```bash
# Evaluate with only the image modality (text is removed)
python evaluate.py --model_path ./saved_models/CNNVGG_D1.pt --dataset D1 --ablation image_only

# Evaluate with only the text modality (image is removed)
python evaluate.py --model_path ./saved_models/CNNVGG_D1.pt --dataset D1 --ablation text_only
```

## Results

Our experiments reveal several key findings:

  * Textual adversarial attacks are significantly more effective at degrading model performance than image-based attacks[cite: 231, 235].
  * LLM-generated attacks, particularly from DeepSeek, are consistently among the most potent, highlighting a new and evolving threat vector[cite: 236, 237].
  * CNNVGG consistently demonstrated the highest overall robustness compared to LSTM-VGG and LongBERT-VGG[cite: 240].
  * Modality ablation studies show a strong, often critical, reliance on visual features, even for text-focused models like BERTVGG[cite: 212, 220].

For detailed performance metrics under all 17 attacks, please refer to Tables 15, 16, and 17 in the Appendix of the paper[cite: 357, 363, 366].

## How to Cite

If you find this work useful in your research, please consider citing our paper:

```bibtex
@article{Jolly2024BeyondTheSurface,
  title   = {Beyond the Surface: Adversarial Stress Testing of Multimodal Fake News Detection Models with LLM-Driven Perturbations},
  author  = {Jolly, Aman and Kumar, Shailender},
  journal = {arXiv preprint},
  year    = {2024}
}
```

## License

This project is licensed under the MIT License. See the `LICENSE` file for details.

## Acknowledgements

This study received institutional support at Delhi Technological University (DTU)[cite: 285].
