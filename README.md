Of course. The provided directory structure and `start.sh` script offer excellent, specific details about how the code is organized and executed. I have updated the `README.md` to reflect this, making the instructions much more precise and actionable.

The main changes include:

  * A new **Code Structure** section explaining the key directories.
  * Updated **Setup and Installation** to include embedding weights.
  * A completely revised **Running the Experiments** section that uses the `.yaml` configuration files and scripts shown in your `start.sh`, instead of placeholder commands.
  * A new **Monitoring Training** section explaining the use of TensorBoard.

Here is the updated `README.md`:

-----

# Beyond the Surface: Adversarial Stress Testing of Multimodal Fake News Detection Models with LLM-Driven Perturbations

This repository contains the official implementation for the paper, "Beyond the Surface: Adversarial Stress Testing of Multimodal Fake News Detection Models with LLM-Driven Perturbations".

In this work, we present a comprehensive and rigorous evaluation of three canonical Multimodal Fake News Detection (MFND) architectures—LongBERT-VGG, CNN-VGG, and LSTM-VGG—across three heterogeneous benchmark datasets. We go beyond conventional evaluation by subjecting these models to a broad suite of 17 adversarial attacks, including novel instruction-tuned Large Language Model (LLM)-generated perturbations from Llama 3 and DeepSeek. Our findings expose critical limitations in current MFND systems and highlight the urgent need for more robust architectures.

[](https://www.google.com/search?q=./FND3_paper_els__Aman_Jolly_Copy_-4.pdf)

## Key Contributions

1.  **Comprehensive Adversarial Benchmark**: We test three canonical MFND architectures (LongBERT-VGG, CNN-VGG, LSTM-VGG) on three diverse datasets (D1, RECOVERY, PolitiFact) against an extensive suite of 17 adversarial attacks.
2.  **Novel LLM-Generated Attacks**: We introduce and evaluate a new class of textual adversarial attacks generated by fine-tuned Llama 3 8B and DeepSeek-LLM 7B Chat models, providing insights into the evolving threat landscape posed by generative AI.
3.  **Systematic Modality Ablation**: We perform a detailed analysis to quantify the impact of removing text or image inputs during evaluation, revealing critical, context-dependent modality dependencies and model vulnerabilities.

## Code Structure

The repository is organized as follows:

```
├── adversarial_attacks/ # Implementation of all 17 text and image attacks.
├── analysis/            # Scripts for result analysis and plotting.
├── configs/             # YAML configuration files for all experiments.
├── data/                # datasets.
├── EMBEDDING_WEIGHTS/   # pre-trained embedding files.
├── logs/                # TensorBoard logs for monitoring experiments.
├── models/              # Model architecture definitions (CNNVGG, LSTMVGG, etc.).
├── nohup_logs/          # Raw output logs from training runs.
├── utils/               # Helper functions and utilities.
├── train.py             # Main script for training models.
├── evaluation.py        # Main script for evaluating models.
└── start.sh             # Script to run all training experiments in sequence.
```

## Setup and Installation

### Prerequisites

  * Python 3.8+
  * PyTorch
  * CUDA-enabled GPU (required for training and attack generation)

### Installation

1.  **Clone the repository:**

    ```bash
    git clone https://github.com/your-username/MFND-Adversarial-Stress-Test.git
    cd MFND-Adversarial-Stress-Test
    ```

2.  **Install dependencies:**
    It is recommended to use a virtual environment.

    ```bash
    pip install -r requirements.txt
    ```

    *(Note: A `requirements.txt` should be created listing all necessary libraries, e.g., `torch`, `torchvision`, `transformers`, `numpy`, `pandas`, `scikit-learn`, `pyyaml`.)*

3.  **Download Datasets:**
    Download the D1, RECOVERY, and PolitiFact datasets from their respective sources (see paper for links) and place them in the `data/` directory, following a `train/val/test` split structure for each.

4.  **Download Embedding Weights:**
    For the CNN and LSTM models, pre-trained word embeddings are required. Create the `EMBEDDING_WEIGHTS` directory and place the necessary embedding files (e.g., GloVe, Word2Vec) inside it.

## Running the Experiments

All experiments are controlled via `.yaml` configuration files located in the `configs/` directory.

### 1\. Model Training

The `train.py` script is used to train all models. You must specify the path to a configuration file. The `start.sh` script provides a template for running all training jobs.

#### Multimodal Model Training (Baseline)

To train a multimodal model, use the corresponding config file.

```bash
# Example: Train CNN-VGG on the D1 dataset
python train.py --config_path configs/multimodal_cnn_D1.yaml

# Example: Train LongBERT-VGG on the RECOVERY dataset
python train.py --config_path configs/multimodal_bert_recovery.yaml
```

#### Modality Ablation Training (Unimodal Models)

The modality ablation analysis is performed by training and evaluating unimodal models.

  * **Text-Only Models:**
    ```bash
    # Example: Train a text-only CNN model on the D1 dataset
    python train.py --config_path configs/unimodal_text_cnn_D1.yaml
    ```
  * **Image-Only Models:**
    ```bash
    # Example: Train an image-only VGG model on the RECOVERY dataset
    python train.py --config_path configs/unimodal_img_vgg_recovery.yaml
    ```

### 2\. Evaluation

The `evaluation.py` script is used to evaluate the performance of trained models on the test sets. The script is designed to run after the models have been trained and saved.

```bash
# Run the full evaluation pipeline
python evaluation.py
```

*(Note: The `evaluation.py` script likely reads model paths and dataset information from the configuration files or a predefined location.)*

### 3\. Adversarial Attack Generation and Evaluation

The code for generating adversarial attacks is located in the `adversarial_attacks/` directory. These scripts can be used to generate perturbed test sets, which can then be evaluated.

```bash
# Example placeholder for generating attacks
python -m adversarial_attacks.run_attack --config configs/attacks/pgd_cnn_d1.yaml
```

*(Note: The exact command may vary. Please refer to the source code in the `adversarial_attacks` directory for specific usage.)*

### 4\. Running All Experiments

The `start.sh` script is provided to launch all training jobs in the background. You can use it as a reference or execute it directly.

```bash
bash start.sh
```

## Monitoring Training

The repository is configured to use TensorBoard for logging. To monitor the training progress:

```bash
tensorboard --logdir=logs/
```

Navigate to `localhost:6006` in your web browser to view training curves, loss, and other metrics.

## How to Cite

If you find this work useful in your research, please consider citing our paper:

```bibtex
@article{Jolly2024BeyondTheSurface,
  title   = {Beyond the Surface: Adversarial Stress Testing of Multimodal Fake News Detection Models with LLM-Driven Perturbations},
  author  = {Jolly, Aman and Kumar, Shailender},
  journal = {arXiv preprint},
  year    = {2024}
}
```

## License

This project is licensed under the MIT License. See the `LICENSE` file for details.

## Acknowledgements

This study received institutional support at Delhi Technological University (DTU).